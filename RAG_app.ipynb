{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgPgJbKfrlW-",
        "outputId": "b08c63c0-6ffb-4a39-fde4-96570074905e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Running in Google Colab, installing requirements.\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Collecting torch\n",
            "  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting triton==2.3.1 (from torch)\n",
            "  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0+cu121\n",
            "    Uninstalling torch-2.3.0+cu121:\n",
            "      Successfully uninstalled torch-2.3.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.3.1 which is incompatible.\n",
            "torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torch-2.3.1 triton-2.3.1\n",
            "Collecting PyMuPDF\n",
            "  Downloading PyMuPDF-1.24.5-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.24.3 (from PyMuPDF)\n",
            "  Downloading PyMuPDFb-1.24.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
            "Successfully installed PyMuPDF-1.24.5 PyMuPDFb-1.24.3\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.0.1\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.31.0\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.43.1\n",
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.5.9.post1.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.3.1)\n",
            "Collecting einops (from flash-attn)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.5.9.post1-cp310-cp310-linux_x86_64.whl size=120889689 sha256=5022ba11d48bf74926da9c16260f4ea2b9bb7f4e29bdb4bd6e1383ad1c55d16f\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/ad/f6/7ccf0238790d6346e9fe622923a76ec218e890d356b9a2754a\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: einops, flash-attn\n",
            "Successfully installed einops-0.8.0 flash-attn-2.5.9.post1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "    print(\"[INFO] Running in Google Colab, installing requirements.\")\n",
        "    !pip install -U torch # requires torch 2.1.1+ (for efficient sdpa implementation)\n",
        "    !pip install PyMuPDF # for reading PDFs with Python\n",
        "    !pip install tqdm # for progress bars\n",
        "    !pip install sentence-transformers # for embedding models\n",
        "    !pip install accelerate # for quantization model loading\n",
        "    !pip install bitsandbytes # for quantizing models (less storage space)\n",
        "    !pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import fitz\n",
        "from spacy.lang.en import English\n",
        "import re\n",
        "import pandas as pd\n",
        "from sentence_transformers import util, SentenceTransformer\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers.utils import is_flash_attn_2_available\n",
        "import textwrap\n",
        "from transformers import BitsAndBytesConfig\n",
        "import random"
      ],
      "metadata": {
        "id": "o_0RI0qSryJA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ae3d1b6-2ebb-4fe5-ba50-034339446b03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"human-nutrition-text.pdf\"\n",
        "\n",
        "if not os.path.exists(pdf_path):\n",
        "  print(\"File doesn't exist, downloading...\")\n",
        "\n",
        "  url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
        "\n",
        "  filename = pdf_path\n",
        "\n",
        "  response = requests.get(url)\n",
        "\n",
        "  if response.status_code == 200:\n",
        "      with open(filename, \"wb\") as file:\n",
        "          file.write(response.content)\n",
        "      print(f\"The file has been downloaded and saved as {filename}\")\n",
        "  else:\n",
        "      print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
        "else:\n",
        "  print(f\"File {pdf_path} exists.\")"
      ],
      "metadata": {
        "id": "Si7iVTxdsBgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f78cdc-7ef9-4825-9fd3-0ad6809dbc46"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File doesn't exist, downloading...\n",
            "The file has been downloaded and saved as human-nutrition-text.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
        "\n",
        "    doc = fitz.open(pdf_path)\n",
        "    pages_and_texts = []\n",
        "    for page_number, page in enumerate(doc):\n",
        "        text = page.get_text()\n",
        "        text = text.replace(\"\\n\", \" \").strip()\n",
        "        pages_and_texts.append({\"page_number\": page_number - 41,\n",
        "                                \"page_char_count\": len(text),\n",
        "                                \"page_word_count\": len(text.split(\" \")),\n",
        "                                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
        "                                \"page_token_count\": len(text) / 4,\n",
        "                                \"text\": text})\n",
        "    return pages_and_texts\n",
        "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)"
      ],
      "metadata": {
        "id": "-mhQ8jhbyNnH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "for item in pages_and_texts:\n",
        "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
        "\n",
        "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
        "\n",
        "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
      ],
      "metadata": {
        "id": "AULZPbL8zEx8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 10\n",
        "\n",
        "def split_list(input_list: list,\n",
        "               chunk_size: int) -> list[list[str]]:\n",
        "\n",
        "    return [input_list[i:i + chunk_size] for i in range(0, len(input_list), chunk_size)]\n",
        "\n",
        "for item in pages_and_texts:\n",
        "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
        "                                         chunk_size=chunk_size)\n",
        "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
      ],
      "metadata": {
        "id": "Hdh6jbNF1fIg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pages_and_chunks = []\n",
        "for item in pages_and_texts:\n",
        "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
        "        chunk_dict = {}\n",
        "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
        "\n",
        "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
        "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk)\n",
        "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
        "\n",
        "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
        "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
        "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
        "\n",
        "        pages_and_chunks.append(chunk_dict)\n",
        "\n",
        "\n",
        "len(pages_and_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3dgT7zi2gdd",
        "outputId": "10e3b45c-60c1-4fdf-e8a2-224961657c55"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1843"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(pages_and_chunks)"
      ],
      "metadata": {
        "id": "OD4x-05r47U7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_token_length = 30\n",
        "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
        "pages_and_chunks_over_min_token_len[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5_rjurG4aRj",
        "outputId": "729111b8-cb87-4a7b-bcc0-089002c35cf1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'page_number': -39,\n",
              "  'sentence_chunk': 'Human Nutrition: 2020 Edition UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM ALAN TITCHENAL, SKYLAR HARA, NOEMI ARCEO CAACBAY, WILLIAM MEINKE-LAU, YA-YUN YANG, MARIE KAINOA FIALKOWSKI REVILLA, JENNIFER DRAPER, GEMADY LANGFELDER, CHERYL GIBBY, CHYNA NICOLE CHUN, AND ALLISON CALABRESE',\n",
              "  'chunk_char_count': 308,\n",
              "  'chunk_word_count': 42,\n",
              "  'chunk_token_count': 77.0},\n",
              " {'page_number': -38,\n",
              "  'sentence_chunk': 'Human Nutrition: 2020 Edition by University of Hawai‘i at Mānoa Food Science and Human Nutrition Program is licensed under a Creative Commons Attribution 4.0 International License, except where otherwise noted.',\n",
              "  'chunk_char_count': 210,\n",
              "  'chunk_word_count': 30,\n",
              "  'chunk_token_count': 52.5}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=\"cuda\")\n",
        "\n",
        "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]\n",
        "text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
        "                                               batch_size=32,\n",
        "                                               convert_to_tensor=True)\n",
        "\n",
        "text_chunk_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22h0-OVm5ltT",
        "outputId": "526c0324-ef23-434f-a076-d6c8a63e2d5b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0674,  0.0902, -0.0051,  ..., -0.0221, -0.0232,  0.0126],\n",
              "        [ 0.0552,  0.0592, -0.0166,  ..., -0.0120, -0.0103,  0.0227],\n",
              "        [ 0.0280,  0.0340, -0.0206,  ..., -0.0054,  0.0213,  0.0313],\n",
              "        ...,\n",
              "        [ 0.0771,  0.0098, -0.0122,  ..., -0.0409, -0.0752, -0.0241],\n",
              "        [ 0.1030, -0.0165,  0.0083,  ..., -0.0574, -0.0283, -0.0295],\n",
              "        [ 0.0864, -0.0125, -0.0113,  ..., -0.0522, -0.0337, -0.0299]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n",
        "embeddings_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
        "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
      ],
      "metadata": {
        "id": "fk6wC2GY6azZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n",
        "text_chunks_and_embedding_df_load.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JmESAzZu8Ix2",
        "outputId": "5a7e286a-2bf1-4baf-96dc-dfe32cc17550"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   page_number                                     sentence_chunk  \\\n",
              "0          -39  Human Nutrition: 2020 Edition UNIVERSITY OF HA...   \n",
              "1          -38  Human Nutrition: 2020 Edition by University of...   \n",
              "2          -37  Contents Preface University of Hawai‘i at Māno...   \n",
              "3          -36  Lifestyles and Nutrition University of Hawai‘i...   \n",
              "4          -35  The Cardiovascular System University of Hawai‘...   \n",
              "\n",
              "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
              "0               308                42              77.00   \n",
              "1               210                30              52.50   \n",
              "2               766               114             191.50   \n",
              "3               941               142             235.25   \n",
              "4               998               152             249.50   \n",
              "\n",
              "                                           embedding  \n",
              "0  [ 6.74242675e-02  9.02281404e-02 -5.09548886e-...  \n",
              "1  [ 5.52156419e-02  5.92139773e-02 -1.66167244e-...  \n",
              "2  [ 2.79801842e-02  3.39813754e-02 -2.06426680e-...  \n",
              "3  [ 6.82566911e-02  3.81275006e-02 -8.46854132e-...  \n",
              "4  [ 3.30264494e-02 -8.49763490e-03  9.57159605e-...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8dbada7d-cfce-440e-bc0d-21716eac40d3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_number</th>\n",
              "      <th>sentence_chunk</th>\n",
              "      <th>chunk_char_count</th>\n",
              "      <th>chunk_word_count</th>\n",
              "      <th>chunk_token_count</th>\n",
              "      <th>embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-39</td>\n",
              "      <td>Human Nutrition: 2020 Edition UNIVERSITY OF HA...</td>\n",
              "      <td>308</td>\n",
              "      <td>42</td>\n",
              "      <td>77.00</td>\n",
              "      <td>[ 6.74242675e-02  9.02281404e-02 -5.09548886e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-38</td>\n",
              "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
              "      <td>210</td>\n",
              "      <td>30</td>\n",
              "      <td>52.50</td>\n",
              "      <td>[ 5.52156419e-02  5.92139773e-02 -1.66167244e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-37</td>\n",
              "      <td>Contents Preface University of Hawai‘i at Māno...</td>\n",
              "      <td>766</td>\n",
              "      <td>114</td>\n",
              "      <td>191.50</td>\n",
              "      <td>[ 2.79801842e-02  3.39813754e-02 -2.06426680e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-36</td>\n",
              "      <td>Lifestyles and Nutrition University of Hawai‘i...</td>\n",
              "      <td>941</td>\n",
              "      <td>142</td>\n",
              "      <td>235.25</td>\n",
              "      <td>[ 6.82566911e-02  3.81275006e-02 -8.46854132e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-35</td>\n",
              "      <td>The Cardiovascular System University of Hawai‘...</td>\n",
              "      <td>998</td>\n",
              "      <td>152</td>\n",
              "      <td>249.50</td>\n",
              "      <td>[ 3.30264494e-02 -8.49763490e-03  9.57159605e-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8dbada7d-cfce-440e-bc0d-21716eac40d3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8dbada7d-cfce-440e-bc0d-21716eac40d3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8dbada7d-cfce-440e-bc0d-21716eac40d3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-825cd245-d4cc-4e89-bd74-5620e7ef69a4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-825cd245-d4cc-4e89-bd74-5620e7ef69a4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-825cd245-d4cc-4e89-bd74-5620e7ef69a4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "text_chunks_and_embedding_df_load",
              "summary": "{\n  \"name\": \"text_chunks_and_embedding_df_load\",\n  \"rows\": 1680,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 349,\n        \"min\": -39,\n        \"max\": 1166,\n        \"num_unique_values\": 1136,\n        \"samples\": [\n          795,\n          918,\n          265\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence_chunk\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1680,\n        \"samples\": [\n          \"The major determinants of Type 2 diabetes that can be changed are overnutrition and a sedentary lifestyle. Therefore, reversing or improving these factors by lifestyle interventions markedly improve the overall health of Type 2 diabetics and lower blood-glucose levels. In fact it has been shown that when people are overweight, losing as little as nine pounds (four kilograms) decreases blood- glucose levels in Type 2 diabetics. The Diabetes Prevention Trial demonstrated that by adhering to a diet containing between 1,200 and 1,800 kilocalories per day with a dietary fat intake goal of less than 25 percent and increasing physical activity to at least 150 minutes per week, people at high risk for Type 2 diabetes achieved a weight loss of 7 percent and significantly decreased their chances of developing Type 2 diabetes.15 The American Diabetes Association (ADA) has a website that provides information and tips for helping diabetics answer the question, \\u201cWhat Can I Eat\\u201d. In regard to carbohydrates the ADA recommends diabetics keep track of the carbohydrates they eat and set a limit. These dietary practices will help keep blood-glucose levels in the target range. Figure 18.5 Metabolic Syndrome: A Combination of Risk Factors Increasing the Chances for Chronic Disease 15.\\u00a0Knowler WC. (2002). Reduction in the Incidence of Type 2 Diabetes with Lifestyle Intervention or Metformin.\",\n          \"Scheme of a micelle formed by phospholipid s in an aqueous solution by Emmanuel Boutet /\\u00a0CC BY-SA 3.0 cholesterol so it acts as an emulsifier. It attracts and holds onto fat while it is simultaneously attracted to and held on to by water. Emulsification increases the surface area of lipids over a thousand- fold, making them more accessible to the digestive enzymes. Once the stomach contents have been emulsified, fat-breaking enzymes work on the triglycerides and diglycerides to sever fatty acids from their glycerol foundations. As pancreatic lipase enters the small intestine, it breaks down the fats into free fatty acids and monoglycerides. Yet again, another hurdle presents itself. How will the fats pass through the watery layer of mucus that coats the absorptive lining of the digestive tract?As before, the answer is bile. Bile salts envelop the fatty acids and monoglycerides to form micelles. Micelles have a fatty acid core with a water-soluble exterior.\",\n          \"Image by\\u00a0Gtirouflet / CC BY-SA 3.0 dense cortical bone is about 10 percent porous and it looks like many concentric circles, similar to the rings in a tree trunk, sandwiched together (Figure 2.27 \\u201cCortical (Compact) Bone\\u201d). Cortical bone tissue makes up approximately 80 percent of the adult skeleton. It surrounds all trabecular tissue and is the only bone tissue in the shafts of long bones. Figure 2.26 The Arrangement of Bone Tissues The two basic tissue types of bones are trabecular and cortical. This photo shows normal (left) and degraded (right) trabecular (spongy) bone. Figure 2.27 Cortical (Compact) Bone. The Skeletal System | 123\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 415,\n        \"min\": 122,\n        \"max\": 1831,\n        \"num_unique_values\": 992,\n        \"samples\": [\n          1467,\n          383,\n          1240\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 66,\n        \"min\": 10,\n        \"max\": 297,\n        \"num_unique_values\": 258,\n        \"samples\": [\n          26,\n          147,\n          138\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 103.80112135479351,\n        \"min\": 30.5,\n        \"max\": 457.75,\n        \"num_unique_values\": 992,\n        \"samples\": [\n          366.75,\n          95.75,\n          310.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1680,\n        \"samples\": [\n          \"[ 2.29729712e-02  2.80858967e-02  1.12715000e-02  2.38459539e-02\\n  6.08849078e-02 -2.20196079e-02 -3.04390900e-02  7.16837496e-02\\n  3.34621929e-02  2.56895032e-02  3.81384008e-02 -1.46272471e-02\\n  6.05134619e-03  3.96036990e-02 -2.14921068e-02 -2.19398756e-02\\n  3.65133472e-02 -3.25683095e-02 -2.36242507e-02  1.27335191e-02\\n -2.04192102e-02 -7.57569866e-03 -2.97014453e-02  1.34395417e-02\\n -3.89803052e-02  7.09829107e-03  8.38490427e-02  4.68919054e-03\\n -6.21600682e-03 -3.64423431e-02  6.83111176e-02  2.30760835e-02\\n  1.70159172e-02 -3.24641615e-02  2.05456968e-06 -3.15726995e-02\\n -4.76174010e-03  2.94138342e-02  9.00705997e-03  4.50910553e-02\\n  1.82500333e-02 -2.45281626e-02  2.80213878e-02  3.24000865e-02\\n  1.67630445e-02 -2.78206356e-02  2.20614839e-02  3.33284363e-02\\n  2.97171902e-02  2.26193946e-02  9.21218190e-03 -9.00363252e-02\\n  1.28435260e-02 -2.76685599e-02 -1.24296751e-02  2.76750606e-02\\n -7.83731788e-03  5.50725460e-02 -3.28327604e-02 -9.84455738e-03\\n  4.40649912e-02 -5.17247468e-02 -2.61238627e-02  2.34586280e-02\\n -3.23815495e-02  1.43910442e-02 -2.31870990e-02 -6.13956004e-02\\n  3.56797464e-02  4.42797542e-02  7.54781887e-02 -1.18830567e-02\\n  2.61459257e-02  2.20207274e-02  1.07243182e-02  9.09299869e-03\\n  2.32546125e-02 -7.01869577e-02 -4.57891934e-02  2.91911066e-02\\n -1.55159086e-02  1.12296343e-02  1.60889253e-02 -2.10474096e-02\\n  1.33295916e-03 -1.47659099e-02 -1.23751191e-02  2.28247084e-02\\n  1.66185517e-02  7.55396718e-03 -2.91870032e-02  2.15626378e-02\\n -4.81806546e-02  2.03475356e-02  2.55755950e-02 -1.04519501e-02\\n  2.39592195e-02  7.22793639e-02  2.07349043e-02 -5.32777049e-02\\n  4.41575311e-02  7.44044557e-02  8.33298713e-02 -2.73018633e-03\\n  2.66782939e-03  1.34550482e-02 -3.98521274e-02  1.67210482e-03\\n -3.05837709e-02  4.47130464e-02 -2.64749187e-03 -1.56471636e-02\\n -7.53284320e-02 -2.73020063e-02  4.55050021e-02 -1.80287682e-03\\n  5.39066736e-03 -4.53817174e-02 -4.69642878e-02 -3.81236225e-02\\n -1.64280459e-02  8.61568097e-03  3.98035981e-02 -1.25875622e-02\\n -1.80445574e-02  1.02986321e-02  4.12722863e-02 -1.31619982e-02\\n  4.11747545e-02  6.24924619e-03  4.69480641e-02 -1.32724438e-02\\n -3.39613925e-03  8.27880949e-03  6.28442178e-03  5.81035987e-02\\n -1.29940435e-02 -3.89739405e-03  2.20918935e-02  7.65071576e-03\\n -5.82268909e-02  5.13123441e-03 -2.03677192e-02 -6.17644452e-02\\n  5.85707976e-03 -2.43919939e-02  3.34543549e-02  3.82698886e-02\\n  3.51353991e-03 -1.31548736e-02 -1.13629512e-02 -1.27255134e-02\\n -2.48163124e-03  1.72944702e-02  8.27016830e-02 -3.42380162e-03\\n -5.70958778e-02  2.50502699e-03  3.03654727e-02  9.44623444e-03\\n  1.72372162e-02  7.99587555e-03 -2.43335757e-02  2.92636547e-02\\n  6.21003769e-02  7.02438236e-04  4.73045446e-02 -2.81685181e-02\\n  3.49310180e-03  5.04241772e-02  1.11960378e-02  3.41657214e-02\\n -4.17057648e-02  5.92101254e-02  2.73968335e-02 -3.56948450e-02\\n  3.93448658e-02  3.51529308e-02  8.42085201e-03 -1.98321957e-02\\n  4.33469713e-02 -1.18261045e-02  4.92545627e-02 -3.05400956e-02\\n  4.26846975e-03  4.03070636e-03 -6.49488345e-02 -7.95924570e-04\\n -3.46954390e-02 -9.99789964e-03  1.64243709e-02 -1.20536881e-02\\n  7.07974732e-02 -9.05370936e-02  2.73720417e-02  2.17130873e-02\\n -2.33670045e-02  9.12264287e-02  1.64063182e-02  3.62658389e-02\\n  1.47561124e-02 -6.63467124e-03 -5.39022982e-02 -6.76799789e-02\\n  4.55670338e-03 -2.33429801e-02  4.05053683e-02 -1.99937429e-02\\n  3.04897185e-02 -5.00026420e-02  4.43863347e-02 -2.04093903e-02\\n  6.49715960e-02 -4.67289835e-02  3.04762907e-02 -4.17238474e-02\\n  1.36049623e-02 -1.18888151e-02  1.12099862e-02  6.29553497e-02\\n -1.77913252e-02  4.15676348e-02 -1.00580053e-02  6.75371150e-04\\n  2.51111519e-02 -1.26866316e-02  1.68014988e-02 -3.28931436e-02\\n -4.90503013e-02 -5.12543647e-03  5.16201593e-02  2.13184338e-02\\n -6.63907155e-02  2.90889200e-02 -7.74796936e-04 -3.93134579e-02\\n -6.18337747e-03  1.16510903e-02 -3.34364139e-02 -2.28617378e-02\\n -9.15383454e-03 -3.65933105e-02 -3.25162262e-02 -1.53120402e-02\\n -9.30079743e-02 -3.57470699e-02 -7.29239685e-03  4.26715985e-02\\n -4.41388600e-02  6.41513467e-02 -2.29487196e-02 -5.17616123e-02\\n  1.37782358e-02  2.89340112e-02 -7.61773959e-02  1.12729799e-02\\n -7.52168596e-02  5.26861101e-03  1.53507972e-02  4.68976758e-02\\n -1.34887900e-02 -8.26931838e-03  5.36376378e-03  4.95160818e-02\\n -1.30892098e-02 -4.93161492e-02  8.89857020e-03  2.88364477e-02\\n -4.04627621e-02 -5.19260094e-02  6.67705247e-03 -2.46643461e-03\\n  2.24981792e-02  2.95400154e-02 -5.24829235e-03  1.17631555e-02\\n  3.35198268e-02 -4.02946165e-03  1.56868231e-02  2.04602368e-02\\n  8.72190893e-02  6.44629896e-02  4.14886996e-02  1.39544327e-02\\n  3.64198275e-02 -4.75281104e-02 -1.10576088e-02 -6.18624985e-02\\n  1.04863308e-02  1.40444888e-03  5.39735444e-02 -1.78427231e-02\\n  4.31339396e-03  6.45331889e-02 -4.83200699e-02  7.58985654e-02\\n  1.35629280e-02 -4.18066978e-02  1.54282255e-02 -2.14080475e-02\\n  2.15211418e-02  3.94406319e-02 -1.00536551e-02  6.46505458e-03\\n -1.72794964e-02 -1.12749003e-02 -2.73868050e-02 -3.50129902e-02\\n  8.66162498e-03 -6.33235043e-03  4.85603251e-02  3.74994241e-02\\n  2.12799814e-02 -2.77581662e-02 -3.15048848e-03 -5.67035610e-03\\n -4.97750714e-02  1.61907412e-02 -3.45674045e-02  2.42027901e-02\\n -6.08441420e-04 -1.02716545e-02 -4.54923660e-02 -1.02981711e-02\\n  1.40911583e-02  3.75180282e-02  1.68752670e-02  2.34776717e-02\\n -6.18587136e-02  3.49496827e-02  2.63891201e-02  3.26851308e-02\\n  1.95784457e-02  4.40052897e-03 -2.89963484e-02 -6.47588400e-03\\n  3.63212861e-02  7.43830502e-02 -2.42742654e-02  5.18779270e-02\\n -7.51687139e-02 -3.40245292e-02  1.70769282e-02  8.19654763e-02\\n -1.98336337e-02 -1.83938295e-02 -5.01794182e-02  2.75631640e-02\\n -4.24477682e-02  2.42094826e-02 -2.18182877e-02  1.33800218e-02\\n  3.36719789e-02  1.84938870e-03  1.08133173e-02 -6.36373088e-02\\n  4.44507152e-02  1.21700894e-02 -2.84682978e-02 -1.32247293e-02\\n  2.05366500e-02  4.44630794e-02 -6.56913072e-02 -1.55691383e-02\\n  1.03134699e-02 -3.58045660e-02 -1.38292583e-02  4.76093061e-04\\n  1.26236770e-02  1.15177697e-02 -8.35120492e-03  3.91691998e-02\\n  3.29118059e-03  1.52778924e-02 -4.11466584e-02 -2.41303574e-02\\n  2.96915621e-02  1.58117022e-02 -5.56087382e-02 -6.59467326e-03\\n  3.99093628e-02 -9.33313295e-02  1.72184650e-02  3.36729432e-03\\n -1.26988860e-02  2.48798970e-02 -8.33463483e-03 -2.17450745e-02\\n  5.75357601e-02 -3.72378267e-02  2.99197137e-02 -5.25633106e-03\\n  7.22941477e-03 -2.23791022e-02  9.92902089e-03 -4.89397943e-02\\n -3.30955125e-02 -3.47236134e-02 -3.55034024e-02  4.07063104e-02\\n  2.99441069e-02 -1.32662177e-01  3.83205079e-02 -2.97528952e-02\\n  1.57118067e-02  1.49636613e-02  3.05667035e-02  5.80923259e-03\\n  4.45674136e-02  8.68415609e-02  3.00537106e-02 -3.14172730e-02\\n -1.12733599e-02 -6.85861632e-02  8.29143077e-02  9.32687253e-04\\n  8.28287005e-03  4.60251383e-02  5.92524000e-02 -8.62023327e-03\\n -1.28702521e-02 -9.42938589e-03  2.76092310e-02  4.46677580e-02\\n -6.61543086e-02 -4.07488719e-02 -2.03002933e-02  2.83709373e-02\\n -2.50594653e-02  1.89823210e-02  6.60287142e-02  6.14814945e-02\\n -1.37754623e-03 -3.10590887e-03  2.10915934e-02  1.09830508e-02\\n -2.95005180e-02 -1.86231025e-02  8.57474133e-02  5.12545519e-02\\n  2.64492128e-02 -3.36265191e-02  3.48860510e-02 -2.85117216e-02\\n  7.39187151e-02  2.89257746e-02  3.45990323e-02 -1.44111356e-02\\n  6.03982285e-02  5.42171765e-03  1.08373733e-02  1.06092310e-02\\n -6.86236396e-02  2.03288117e-04  1.60011351e-02 -8.57141428e-03\\n -2.39313841e-02 -2.35607326e-02 -3.61981764e-02 -4.08706516e-02\\n  1.07472815e-01  3.81626049e-03  3.19191143e-02 -1.63456071e-02\\n -1.62466131e-02 -6.20278204e-03 -5.22550605e-02  5.99559210e-02\\n -5.44447228e-02 -1.31396269e-02  4.59873267e-02  4.86567691e-02\\n  2.39443686e-02  3.04191876e-02 -4.35936712e-02 -5.25394008e-02\\n -7.88886696e-02  2.50433222e-03 -3.06024663e-02 -3.53882685e-02\\n -4.16593067e-02  2.38333158e-02 -1.45164807e-03 -2.84110345e-02\\n -2.23766919e-02  3.99736091e-02 -1.79760810e-02 -2.58030556e-02\\n -2.83792131e-02  1.75174233e-02 -3.72972079e-02 -2.12674402e-03\\n -6.22427501e-02 -6.65185601e-02 -2.23804079e-02 -7.89400488e-02\\n -1.83512717e-02  5.61043695e-02  1.92721095e-02  5.66429757e-02\\n  6.06254116e-03 -2.40686052e-02  3.77018452e-02 -2.64845230e-02\\n -3.40272952e-03 -3.50038707e-02 -1.39413746e-02  1.73181354e-03\\n  2.22160127e-02  3.72691378e-02 -3.77214104e-02  4.42813803e-03\\n  1.78216752e-02 -2.63886321e-02  1.44201498e-02  5.36084129e-03\\n -1.14303408e-02  1.87312569e-02  1.34985230e-03 -4.87835286e-03\\n -3.91572108e-03 -1.70022659e-02  5.42912353e-03  6.04774803e-02\\n  1.41849415e-02  3.82715948e-02 -3.95067558e-02 -2.00121272e-02\\n -2.09616944e-02  2.74659786e-02  9.52143967e-03 -6.81963516e-03\\n -1.10616554e-02 -8.06704164e-03 -2.63257436e-02 -1.13474466e-02\\n  4.03615721e-02 -2.57180873e-02 -3.69606055e-02 -2.36007608e-02\\n  1.45106716e-02 -3.04713380e-02  2.86728740e-02 -2.93919444e-03\\n -2.37204973e-03 -8.02426413e-03 -3.76460627e-02 -2.02637110e-02\\n  2.97035687e-02 -2.50616148e-02  1.87683746e-03 -5.57896569e-02\\n -2.73329560e-02 -3.78011167e-02  1.61891244e-02 -5.38802897e-33\\n  2.96111181e-02 -9.83624719e-03  4.98379879e-02  5.77777326e-02\\n  5.10296822e-02  1.32933445e-02 -6.45377636e-02 -9.37822554e-03\\n -5.93022890e-02  7.89666101e-02 -2.94902213e-02 -2.31253002e-02\\n  2.72985399e-05 -4.59786952e-02 -6.16562590e-02 -2.65489593e-02\\n  7.99557474e-03  1.57606276e-03 -3.19293663e-02 -1.06759686e-02\\n -6.44516200e-02  3.45161185e-02  8.73177964e-03  1.91062242e-02\\n  2.99177933e-02  4.72093485e-02  1.41662303e-02 -5.85156435e-04\\n  1.50941033e-03  1.84304677e-02 -1.34501401e-02  7.77282864e-02\\n -3.04925285e-04  4.56190482e-02  1.24297431e-02  7.88352489e-02\\n -6.26035081e-03 -3.03326119e-02  2.22761407e-02  3.11824074e-03\\n  6.48971424e-02 -5.33699654e-02  1.14038214e-03  3.22002592e-03\\n -8.58834088e-02  7.39832669e-02 -2.24174149e-02 -1.98693629e-02\\n  2.02376978e-03  2.72148270e-02 -4.08967547e-02 -6.27453765e-03\\n  2.47814786e-02  4.25336882e-02 -2.33338978e-02  4.21236120e-02\\n -5.35760261e-03 -1.38929300e-02  2.34857071e-02  2.73593366e-02\\n -2.59762164e-02  3.09628043e-02 -4.41264287e-02  3.08858417e-02\\n -3.02760443e-03 -7.87444599e-03 -1.53053449e-02  1.24025848e-02\\n -1.44549301e-02 -7.57972375e-02 -7.31532797e-02 -6.25108033e-02\\n  2.19611656e-02  4.04325128e-02 -2.31300900e-03 -6.09265566e-02\\n -4.90598902e-02  1.80721302e-02 -4.54713255e-02 -4.93626110e-02\\n -9.59483627e-03 -1.43828010e-02  1.51193840e-02 -3.79985310e-02\\n -4.98143584e-02  7.65519217e-02 -3.22400853e-02 -3.91259827e-02\\n -7.93610960e-02 -3.08242645e-02  2.04933472e-02  9.77791287e-03\\n -4.97575093e-04  5.28453244e-03  4.71585430e-02  5.64918062e-03\\n  8.42629075e-02 -6.12572692e-02 -3.29317059e-03  7.30867079e-03\\n -1.97428428e-02 -1.43558551e-02  1.55156280e-03  4.72506173e-02\\n -2.23656278e-03  2.66999938e-02 -1.06265480e-02 -2.58234162e-02\\n  7.00479820e-02 -1.86981261e-02 -3.37645672e-02  2.94382703e-02\\n -3.21478657e-02 -3.84805053e-02 -5.57591282e-02  4.19322774e-02\\n  2.99479533e-02  8.49122629e-02  1.71478745e-02 -5.41984625e-02\\n -3.17496471e-02  3.15846279e-02  3.04570980e-02 -3.83366346e-02\\n -6.88592112e-03 -1.95200220e-02  3.14677991e-02 -1.13854895e-03\\n -7.36938119e-02 -1.74842868e-02  5.41744661e-03 -1.92077104e-02\\n  2.78682137e-07  4.08470184e-02 -1.53628632e-03 -1.78423189e-02\\n -1.03535233e-02 -3.08404192e-02 -6.02940517e-03  3.50857563e-02\\n -6.51571378e-02 -5.80149330e-02  4.58920673e-02  4.16008011e-02\\n -3.90204564e-02 -3.64808962e-02 -3.20155025e-02  6.35178387e-02\\n  2.17223652e-02  1.82709042e-02 -1.36412205e-02 -4.86772275e-03\\n -7.65325595e-03  4.14719339e-03  3.67409103e-02 -2.42217444e-02\\n -7.34255984e-02 -3.96182053e-02  4.99258470e-03  4.51597758e-03\\n -4.24852669e-02  6.33915886e-04 -2.84216926e-02  9.66380630e-03\\n  8.96294974e-03  4.65004295e-02 -7.67296329e-02 -2.04632115e-02\\n -7.31814653e-02 -6.51863143e-02  6.90150857e-02 -3.71697284e-02\\n  4.29911762e-02 -1.47519633e-02  1.61680002e-02 -1.68805681e-02\\n -4.27693613e-02 -3.91147472e-03 -5.42811714e-02 -4.86369655e-02\\n  4.49501313e-02  2.64735520e-02 -7.76932463e-02  4.56957985e-03\\n  2.51250882e-02 -5.15799671e-02  9.67912097e-03 -1.23087047e-02\\n  2.34425999e-02  5.72338793e-03  3.40899415e-02  2.72950362e-02\\n -3.42273489e-02 -3.70479841e-03 -5.08046616e-03  3.87148373e-02\\n -8.21772218e-03 -5.54903001e-02 -7.95374159e-04  1.96648911e-02\\n  1.80299746e-34  1.06232800e-02  7.13028107e-03 -8.23647343e-03\\n -4.72389422e-02  1.98795907e-02 -8.45584832e-03 -1.23929149e-02\\n  1.10524462e-03 -2.05322794e-04  2.58644149e-02 -6.45051524e-02]\",\n          \"[-2.89910735e-04 -4.76907492e-02  4.69394773e-02 -1.34329256e-02\\n  2.12686360e-02 -6.53087068e-03  1.84320193e-02 -5.08894678e-03\\n  4.43609543e-02 -2.93549653e-02  1.95518564e-02  5.42302988e-02\\n  1.03220148e-02 -2.42334884e-02  4.03822921e-02 -3.05171981e-02\\n -2.09650956e-02  7.83721544e-03 -1.09209083e-01 -1.38158966e-02\\n -1.22020207e-02  3.17832944e-03 -2.41586682e-03  3.28726098e-02\\n  6.40644804e-02 -3.99281606e-02 -1.83247719e-02 -3.14286165e-02\\n -4.16912213e-02 -3.85700837e-02 -1.26829343e-02 -1.41209865e-03\\n  1.31108062e-02 -9.94675606e-03  2.10254871e-06 -7.18540251e-02\\n -5.03428802e-02  2.97871716e-02 -1.69740599e-02 -2.29916032e-02\\n -1.15232810e-03 -9.93641540e-02 -1.35236466e-02 -7.95061979e-03\\n  3.42773981e-02 -5.95667362e-02 -1.08887609e-02 -1.95660256e-02\\n  7.71187916e-02  8.48048739e-03 -8.23518354e-03 -7.42352158e-02\\n  3.04884110e-02  5.15961759e-02  5.65598123e-02 -2.05483083e-02\\n -5.42607950e-03 -2.55838782e-02 -3.91943641e-02 -3.48252021e-02\\n  1.56397428e-02  5.52787483e-02  2.22797859e-02 -2.18725502e-02\\n  1.79669056e-02  6.62144572e-02  1.44304838e-02 -3.41766737e-02\\n  3.90865318e-02 -2.13876888e-02 -7.50386193e-02 -3.48831303e-02\\n -3.81169505e-02  1.10502150e-02 -4.12468053e-02  1.77212786e-02\\n  6.79142866e-03  1.09248254e-02  2.70897709e-02  1.80689208e-02\\n  6.10432960e-02  1.14617050e-02 -3.80260847e-03 -3.18495855e-02\\n  1.48276682e-03 -1.43926553e-02 -1.98453534e-02 -4.45456058e-02\\n  7.31130037e-03 -5.73883876e-02  5.91200357e-03 -3.44004631e-02\\n  4.99224756e-03  3.59584615e-02  6.44462276e-03  1.80018116e-02\\n  6.66555017e-02 -1.43214915e-04  5.16850278e-02 -2.70861341e-03\\n -8.12202320e-03 -3.59169021e-02 -2.43293568e-02  3.56825888e-02\\n  3.06037888e-02  2.07295101e-02 -2.72049010e-02  3.89792062e-02\\n -2.48995889e-03  2.48275232e-02  1.78018268e-02 -2.56232731e-02\\n  3.13616432e-02  7.06909299e-02 -3.29235606e-02  6.67960197e-03\\n  1.89117144e-03 -1.15451925e-02 -1.72486249e-02  3.75370458e-02\\n  8.91311280e-03  3.62646990e-02  3.51427384e-02  1.90210883e-02\\n -4.58955057e-02  1.19934686e-01  3.36185768e-02 -7.39446096e-03\\n -1.03651602e-02  8.12049881e-02  4.01631668e-02  1.02531221e-02\\n -4.94023971e-02 -1.05537288e-02 -2.22280361e-02  3.68791493e-03\\n -1.05225015e-02 -2.50414722e-02 -3.70037444e-02 -4.52544652e-02\\n  7.80267594e-03  3.66311371e-02  2.23718900e-02  5.00968890e-03\\n -2.80605908e-02 -1.19902072e-02  1.19607532e-02  3.20545770e-02\\n  2.95469817e-03  6.01988584e-02  2.25247932e-03  6.14434443e-02\\n -2.30831411e-02  1.14265140e-02  2.39354055e-02  1.05365543e-02\\n -1.95968766e-02 -6.21460844e-03 -3.88051420e-02  1.25449747e-02\\n  4.87881489e-02 -1.30138760e-02  3.37267518e-02 -2.83177737e-02\\n -3.69780660e-02 -1.87966954e-02  2.91273575e-02  3.03810872e-02\\n -2.06773151e-02  1.28035024e-02  3.55234668e-02  5.05655212e-03\\n  3.86969410e-02 -6.59883097e-02  6.12658076e-03  6.19648397e-02\\n -7.87231326e-03  2.65277247e-03 -5.22707403e-02  8.05527344e-03\\n  8.70954469e-02  5.17622009e-02  2.65399069e-02 -1.25501503e-03\\n  1.99715979e-02  2.72730663e-02  4.49447669e-02  3.09776962e-02\\n -3.58908772e-02  1.70547906e-02  1.96558908e-02 -1.12501793e-02\\n -5.03951497e-03 -5.86961955e-02 -9.83590959e-04  4.27694917e-02\\n -5.07055633e-02 -1.89975593e-02  4.01682733e-03  3.92593034e-02\\n  1.55548693e-03  7.46860541e-03  5.32642640e-02 -4.59342403e-03\\n -2.41951030e-02  5.40453196e-02 -7.32910186e-02 -6.24243878e-02\\n  1.83400027e-02 -8.10545012e-02 -4.10038345e-02 -5.79014830e-02\\n  5.77441463e-03 -1.38999149e-02  3.55744734e-02  6.65484229e-03\\n  3.30575407e-02  1.69161614e-02 -1.81860868e-02  3.19372416e-02\\n -1.55437961e-02  5.05150668e-02  5.82077727e-02 -2.34185979e-02\\n  5.89606073e-03 -2.15327907e-02  4.05314192e-03 -1.02185132e-02\\n -4.35367711e-02 -4.99300100e-03  1.46756461e-02 -1.71298329e-02\\n  1.82500705e-02  1.11141447e-02 -7.75681529e-03 -3.55587192e-02\\n  7.47965276e-03 -8.88304040e-03 -2.59020030e-02  6.83148857e-03\\n -4.77917828e-02 -2.46858783e-02 -3.94188752e-03  1.66137014e-02\\n  7.24267634e-03 -1.85808558e-02 -6.14708364e-02  1.26165347e-02\\n -3.51574942e-02  1.67146046e-02 -3.57484780e-02 -4.81097698e-02\\n -6.50256276e-02 -3.60029191e-02 -9.18580964e-03  4.35761511e-02\\n  6.52618706e-02 -1.07990149e-02  8.77566729e-03 -6.16241572e-03\\n -1.77497417e-02  2.54069455e-02  3.35899442e-02 -1.85800437e-02\\n -1.61337592e-02 -4.13477346e-02 -1.49374781e-02  2.46960893e-02\\n  2.41081249e-02  9.61841142e-04  3.87678668e-02  3.26361246e-02\\n  6.86734822e-03  5.00897691e-02  5.11830188e-02 -4.05327789e-02\\n -8.97689760e-02 -8.13838269e-04 -3.85215655e-02  1.96412746e-02\\n -4.87155095e-03  6.26533031e-02 -5.00203371e-02  1.36259310e-02\\n  1.98138170e-02  2.23706998e-02 -1.47624556e-02 -2.08031237e-02\\n  5.73374741e-02 -1.08853631e-01  7.27196187e-02  3.57444026e-02\\n -1.71732216e-03 -4.48276754e-03 -1.11867022e-02 -1.15359081e-02\\n -6.34977967e-03  6.09585308e-02  1.82274189e-02  8.02924335e-02\\n -6.22956501e-03  2.27490924e-02  2.69049909e-02  1.22414818e-02\\n -3.83753851e-02  5.28007969e-02 -2.88848113e-02  4.91494825e-03\\n  5.87584712e-02  4.09483258e-03  1.61685962e-02  1.02570588e-02\\n -5.05571030e-02  5.99424243e-02  2.43418068e-02 -2.55821459e-02\\n  3.37801017e-02  2.05735844e-02 -2.93751583e-02 -3.52503918e-02\\n  2.85072997e-02  2.23733932e-02 -1.63354613e-02  1.31281624e-02\\n -2.06889194e-02  1.05702206e-02  2.39639115e-02  2.64977408e-03\\n -4.38604094e-02 -2.48857718e-02 -5.12024667e-03 -4.29650396e-02\\n  3.62291187e-02  1.21394852e-02 -2.53850687e-02 -4.19798791e-02\\n -2.53918786e-02  3.80630642e-02 -1.32844523e-02 -6.39878362e-02\\n -6.17671805e-03 -2.35546101e-03  2.35471465e-02  5.53486682e-02\\n  3.34822424e-02 -6.05285168e-03  3.04024760e-02  4.34461422e-03\\n -3.28185409e-02 -8.15888401e-03 -9.93102323e-03 -4.69345376e-02\\n -2.77412869e-02 -4.12742235e-02 -5.05772792e-02 -2.53928006e-02\\n -7.41897238e-05  2.39503905e-02 -1.62669905e-02 -1.58133835e-03\\n  1.10625632e-01  1.11785065e-02  2.37951316e-02 -6.20768294e-02\\n  4.65054922e-02  4.82486039e-02  5.55227557e-03  3.66428234e-02\\n  5.40087791e-03 -1.73931941e-02 -5.17673744e-03  1.78117566e-02\\n -3.30749415e-02  3.81597057e-02  9.95533075e-03 -2.02581962e-03\\n  6.38610404e-03  4.95962463e-02 -8.83316472e-02 -6.28763228e-04\\n  8.45937710e-03  1.88548472e-02  1.00364130e-04 -1.25207026e-02\\n -7.24978046e-03 -2.52529141e-03  3.03694909e-03 -2.31710598e-02\\n  1.31589556e-02  4.16985825e-02 -9.28908773e-03 -5.84791750e-02\\n -6.46011680e-02 -9.03794989e-02  1.03680491e-02 -1.47697348e-02\\n  3.39405611e-02  2.08894573e-02 -2.78323218e-02  6.83821589e-02\\n  3.50590236e-02 -4.68555689e-02  4.72042076e-02 -3.77112441e-03\\n -5.75096253e-03  2.41767466e-02 -4.17235941e-02  2.28880756e-02\\n  4.02758233e-02 -4.91468981e-02  5.05239628e-02 -1.00680158e-01\\n -2.90744472e-02 -1.67413708e-02  1.17921848e-02 -1.26427915e-02\\n  2.48180218e-02  2.69938000e-02 -9.13031306e-03 -5.64103164e-02\\n  4.68287803e-02  3.84604670e-02  3.81848142e-02  2.33297888e-02\\n  2.69803610e-02 -3.96660902e-02 -2.58250758e-02 -6.80448487e-02\\n -7.80719274e-04 -1.27612036e-02  2.29762010e-02 -1.66850742e-02\\n -1.99029464e-02  1.26399044e-02  9.90941525e-02 -4.01808042e-03\\n  4.98764403e-02 -5.11977188e-02  1.49696544e-02  3.19837220e-02\\n  2.93051992e-02 -3.64751816e-02  3.42510156e-02 -6.13177707e-03\\n  6.81453645e-02 -2.24678498e-03 -1.98654179e-02  5.55717237e-02\\n -2.85562035e-02  3.58595587e-02 -1.89085081e-02  3.27889845e-02\\n -3.52927782e-02  4.49295454e-02  3.33935805e-02 -2.24282476e-03\\n  1.83846429e-02  4.99155652e-03  2.87550557e-02 -6.33753240e-02\\n  2.02281289e-02 -2.66907533e-04  3.94245014e-02 -1.10258274e-02\\n -3.18046138e-02 -1.13525372e-02 -2.47585289e-02 -4.52763459e-04\\n -1.15936622e-02 -9.17788537e-04 -4.71309796e-02  3.67310457e-02\\n -2.05483865e-02  7.07767438e-03 -3.64862531e-02 -6.40820637e-02\\n -5.00232242e-02 -2.14895923e-02 -2.11248789e-02  2.18097661e-02\\n -3.70296799e-02 -7.97156896e-03 -7.28307292e-04 -5.57386354e-02\\n  1.37111153e-02  3.24638449e-02 -4.11842763e-02 -8.94591399e-03\\n -2.66409963e-02 -4.38091956e-04  7.27507696e-02  3.20167057e-02\\n  7.19509600e-03 -7.69989118e-02 -8.38134252e-03 -1.57920923e-02\\n  1.48548437e-02  3.00976424e-03  1.15479948e-02  7.29491115e-02\\n -1.33349663e-02 -1.48559851e-03  7.86755513e-03  1.31217875e-02\\n -7.11150914e-02 -1.64781269e-02 -1.93814710e-02 -1.98638737e-02\\n -3.71746086e-02  6.20009191e-02  3.73834372e-02 -1.30262449e-02\\n -9.81295332e-02  4.18129601e-02  1.00712385e-02 -8.76897667e-03\\n -3.06629203e-03 -7.13661686e-02  4.43297476e-02  2.85195205e-02\\n  7.60584278e-03 -1.95184536e-02 -3.53755690e-02 -1.98184811e-02\\n  1.83212617e-03  3.23965028e-02  7.32532330e-03  1.30306305e-02\\n -6.46259710e-02  6.06432855e-02 -2.28145029e-02 -9.53818951e-03\\n -2.75356565e-02 -1.14284353e-02  1.32799949e-02 -2.13443078e-02\\n  6.87101409e-02  5.53869046e-02  2.36709360e-02 -5.58103174e-02\\n -1.48388359e-03  1.56297795e-02 -2.13469844e-02  5.28876260e-02\\n  3.98664139e-02  2.78922580e-02 -2.24801544e-02  2.37328303e-03\\n  5.89274503e-02 -5.76467775e-02 -2.99277268e-02  1.71148032e-02\\n -5.72593212e-02 -4.33231285e-03  4.52495329e-02 -5.64960070e-33\\n  7.47269094e-02  3.62142138e-02  3.95277478e-02  3.07968576e-02\\n  5.90636693e-02  3.75339501e-02 -2.31895726e-02 -4.05858383e-02\\n -1.92534924e-03  9.35129151e-02 -5.24862250e-03 -1.54407015e-02\\n  9.47384071e-03  2.04024091e-02 -5.42762764e-02 -8.57644305e-02\\n  3.89174446e-02 -2.27755848e-02  4.55556773e-02  2.87668332e-02\\n -4.85455543e-02 -6.96453229e-02 -8.42806175e-02 -2.75642015e-02\\n  7.34933466e-03 -2.87090186e-02  6.42396212e-02 -6.70448085e-03\\n -9.58653986e-02  1.88550483e-02 -3.90135646e-02 -3.27156186e-02\\n -4.77096625e-02 -2.81655490e-02 -1.86207108e-02  2.91774161e-02\\n  2.21861750e-02 -1.15162283e-02 -2.01660730e-02  1.75863020e-02\\n -2.69363285e-03 -2.08921097e-02  1.97621379e-02 -4.78858082e-03\\n -1.26683619e-02  1.35948882e-02  1.80629622e-02  3.39837112e-02\\n  2.93650609e-02 -2.20210832e-02 -1.12774428e-02  2.52639689e-02\\n -3.16103883e-02 -3.53645737e-05  7.28631169e-02  9.46151465e-03\\n  5.21957800e-02 -6.56996295e-02  9.55659822e-02  8.91741831e-03\\n -4.13986333e-02 -1.96979428e-03 -2.69607920e-02 -3.09552792e-02\\n -3.77707742e-02 -2.05183239e-03  2.14984917e-04  6.11692369e-02\\n  7.54572675e-02  3.39427218e-02 -5.28387493e-03 -8.06459859e-02\\n  1.36984903e-02 -9.93240699e-02  1.18184602e-02 -1.05019547e-02\\n -1.66479088e-02 -5.94034381e-02 -7.50455484e-02 -9.63898599e-02\\n -4.96468060e-02 -3.80980363e-03  2.93504149e-02 -1.30302366e-02\\n -1.96876731e-02  7.66343391e-03 -6.62882987e-04 -6.74138777e-03\\n -3.01071852e-02 -3.74916419e-02  8.14801380e-02  1.02742910e-02\\n -3.63987908e-02  4.70368713e-02  4.34757471e-02 -6.05085008e-02\\n  5.91272563e-02 -6.01011366e-02 -4.68130084e-03  5.12216762e-02\\n -1.28850369e-02  1.57379806e-02  1.33342706e-02  2.65763644e-02\\n -5.83724044e-02  3.47832330e-02 -1.22272791e-02  1.50706095e-03\\n -1.26263043e-02 -2.35722829e-02 -8.27669539e-03  3.78872566e-02\\n -1.11825429e-02 -2.38841549e-02 -1.60250515e-02  3.18535157e-02\\n  5.17347408e-03 -9.09908582e-03 -6.33173902e-03 -4.04079333e-02\\n -1.26681169e-02  2.52131280e-03 -4.30412777e-02  9.42817144e-03\\n -6.64179102e-02 -2.53620930e-02  3.53503413e-03  8.19246657e-03\\n  5.27810073e-04 -4.34151255e-02 -3.94068286e-03 -1.35703934e-02\\n  2.88146026e-07  4.27781008e-02  5.02584130e-03  7.52717210e-03\\n  1.53221963e-02 -3.81950592e-03 -4.28856388e-02  4.52936767e-03\\n  1.84004679e-02 -6.30369643e-03 -2.81434022e-02 -1.63980434e-03\\n  3.87146894e-04 -2.06686463e-02  3.05093341e-02  5.44758840e-03\\n  5.98083548e-02  5.66420220e-02 -2.37631034e-02  3.25634168e-03\\n  4.02556248e-02  2.62154248e-02  2.93611716e-02  4.04729322e-02\\n -3.56091261e-02 -3.55419237e-03  7.23572969e-02 -5.21490350e-02\\n  3.87458056e-02  6.35161027e-02 -3.82685214e-02 -1.84759367e-02\\n  1.70838181e-02  7.56823570e-02 -5.08912429e-02 -1.48347579e-03\\n -2.67082378e-02  6.18557027e-03 -8.99394450e-04 -1.19499853e-02\\n -7.07405107e-03  3.20798010e-02  1.72325242e-02 -1.53200990e-02\\n  7.72961415e-03 -5.85321561e-02 -3.12942429e-03  5.66900382e-03\\n -2.73432378e-02  3.62395383e-02 -1.12157129e-02 -1.35082491e-02\\n  3.58749926e-02  6.22950634e-03  2.90324874e-02 -9.97451413e-03\\n -1.87331941e-02  3.08613442e-02  2.58529987e-02 -1.21459886e-02\\n  2.73999870e-02  1.34400213e-02  8.27934500e-03 -4.50032167e-02\\n  1.50194066e-02  5.98297454e-02 -6.39654621e-02 -3.34206223e-02\\n  3.10314759e-34 -3.95918760e-04 -6.39823824e-02 -1.33804725e-02\\n -3.26808076e-03  4.86219535e-03 -3.79619971e-02  3.63107920e-02\\n  3.06082554e-02 -7.84143514e-04  5.17915264e-02 -1.50401648e-02]\",\n          \"[ 1.57728139e-02 -5.64577729e-02 -3.63888033e-03  1.73159987e-02\\n -2.66745500e-03  2.10536420e-02 -2.02101655e-02  4.30584364e-02\\n -3.53951268e-02  2.19664518e-02 -4.33511287e-02 -5.20750508e-02\\n -5.89872338e-03  2.55817883e-02  3.15636769e-02  7.17286533e-03\\n -3.72072756e-02 -1.11364517e-02 -1.11601995e-02 -3.12545337e-02\\n -3.86702456e-02 -2.43350528e-02 -4.18420956e-02 -4.27627712e-02\\n -3.75365429e-02 -1.38184447e-02  2.48206146e-02 -1.15433261e-02\\n -1.36964619e-02 -3.28309881e-03  1.13740982e-02 -1.04402518e-02\\n  5.59672639e-02 -3.27690318e-02  2.05511310e-06  8.69802199e-03\\n  5.24926791e-03 -6.02529326e-04  7.95765501e-03  8.60197842e-02\\n  1.62341874e-02  3.49696493e-04 -2.19876915e-02 -1.75113545e-03\\n  2.28333678e-02  2.16686577e-02 -1.60330608e-02  2.73146927e-02\\n -6.92030713e-02 -6.68439129e-03 -2.49029859e-03  2.01713145e-02\\n  1.47406431e-02  1.52928270e-02  6.65228488e-03  2.65217684e-02\\n -5.44198453e-02  1.34172156e-01 -1.52043123e-02  2.51244586e-02\\n -2.14611422e-02  7.70719722e-04  1.96670070e-02 -2.68957503e-02\\n  4.74147126e-02  4.20907736e-02  4.15420420e-02 -4.86595370e-02\\n  2.95606162e-02  1.16407927e-02  8.50693509e-03 -1.01046441e-02\\n  4.82071489e-02  6.38101064e-03  2.07358552e-03 -5.98762743e-02\\n -1.56382807e-02  4.81903777e-02  2.05990300e-02 -7.43976096e-04\\n  1.42410239e-02  3.69130692e-04 -3.08749732e-02  5.07627055e-03\\n -1.63760204e-02 -8.42410028e-02  3.60810347e-02  8.39669071e-03\\n  5.14656641e-02 -3.27205695e-02  7.42358039e-04 -3.66088492e-03\\n  1.47049148e-02  4.50345390e-02 -6.67315498e-02 -3.50497290e-02\\n -2.04252452e-03 -1.14344561e-03  1.14085399e-01 -3.33110057e-02\\n -5.43336533e-02 -6.90692663e-03 -1.05299480e-01  4.72107492e-02\\n -7.59746283e-02 -4.97120246e-02 -4.04516011e-02  2.93611512e-02\\n  1.07014980e-02 -3.83841433e-03  3.03368866e-02  4.20745462e-03\\n  1.17355296e-02  3.46791558e-02 -2.05858164e-02 -6.12229370e-02\\n -2.72969250e-02 -4.43289801e-03  7.09903007e-03 -2.08244193e-02\\n -6.91965455e-03  6.64384440e-02  1.79156717e-02  2.68767122e-02\\n  1.39488243e-02 -5.40009560e-03 -3.38539816e-02  1.90435927e-02\\n  9.69934836e-03 -2.47601252e-02  1.38934590e-02  3.50373574e-02\\n -1.62765086e-02  6.22384483e-03 -2.01239274e-03 -7.44358748e-02\\n  5.14437035e-02  5.49384486e-03  1.21744294e-02  3.73239219e-02\\n -1.50537109e-02 -3.23126055e-02 -1.56131070e-02 -3.34043358e-03\\n -1.84459947e-02  2.07658317e-02 -1.68162771e-02  6.05426207e-02\\n -9.68633406e-03  8.01254530e-03 -2.72264406e-02 -4.52809175e-03\\n -7.75353014e-02 -2.35011261e-02  5.10917678e-02 -3.80758867e-02\\n  1.37859508e-02 -1.74623784e-02  3.25980298e-02  5.16020320e-02\\n  2.20756009e-02 -2.48685982e-02 -5.07828332e-02 -7.71802366e-02\\n  6.98580779e-03 -1.88843627e-02 -4.38863672e-02  1.05715198e-02\\n  3.30445208e-02  3.87548818e-03  4.17918675e-02  7.42689371e-02\\n -1.62473880e-02 -2.62174979e-02 -2.07489152e-02  1.32539898e-01\\n -4.11955602e-02  8.61665234e-04  6.24621920e-02 -7.81116262e-02\\n  7.59768039e-02 -2.26889178e-02  4.39216346e-02 -2.91420631e-02\\n  3.70258503e-02 -6.29993854e-03 -1.81148071e-02  5.53550711e-03\\n  1.04619013e-02 -3.58778760e-02 -7.27115991e-03 -4.91430573e-02\\n -1.79034621e-02  2.63426341e-02  2.13690829e-02 -1.68986395e-02\\n  6.72692084e-04 -7.07057565e-02 -1.83181074e-02  2.31532566e-02\\n -3.04878112e-02 -3.46204429e-03 -5.81701845e-03 -5.40998206e-02\\n -4.43706177e-02 -4.23483402e-02 -4.37021367e-02  2.59232540e-02\\n  3.26459343e-03  2.61602327e-02 -2.89005805e-02  3.05306297e-02\\n  6.89620385e-03 -4.21663038e-02  1.18978396e-02 -1.43660037e-02\\n  1.15575418e-02 -2.52995882e-02 -4.00522910e-02  1.20739350e-02\\n -2.85350680e-02 -1.07138921e-02 -3.22781019e-02  8.96959193e-03\\n -4.03619893e-02  8.41991827e-02  1.07750827e-02 -3.76433507e-02\\n -7.40152821e-02 -4.36559580e-02  2.69117989e-02 -1.00799685e-03\\n  3.17117423e-02  5.25654964e-02 -4.80853803e-02 -5.99735826e-02\\n -8.01319331e-02  4.34510373e-02 -1.74203720e-02  4.51894887e-02\\n  1.31648295e-02 -9.69533473e-02  4.54950668e-02  4.50390577e-03\\n  4.80539836e-02 -2.98802126e-02  2.00214162e-02  2.06043832e-02\\n -1.51534937e-02 -1.61411949e-02 -3.15559916e-02 -4.95321713e-02\\n  4.67078015e-03  2.07061321e-02 -7.21533224e-02  4.37031500e-02\\n -9.53316763e-02  1.38542540e-02 -5.00775017e-02 -2.00575031e-02\\n  4.04674932e-02 -4.60864492e-02  1.04648452e-02 -2.10422873e-02\\n -1.63541567e-02  2.04278179e-03  1.52302468e-02 -7.33365677e-03\\n  6.12386083e-03  1.80253889e-02 -3.73194413e-03  4.54135388e-02\\n -1.63549334e-02  2.90952753e-02 -5.81544563e-02  1.81123242e-02\\n  4.74347919e-02  3.09087802e-02 -4.07738751e-03  3.95797528e-02\\n  6.03918172e-03  7.78303444e-02 -7.49446228e-02 -2.59119626e-02\\n  1.99973714e-02 -3.86637338e-02 -5.09800501e-02 -3.81934792e-02\\n  3.10432483e-02  6.50127158e-02  5.39996009e-03  6.04327917e-02\\n -3.42034153e-03 -2.26868819e-02  3.74620222e-02  3.57151777e-03\\n -1.46530364e-02  2.38615051e-02  1.38629163e-02  7.55867735e-02\\n  9.34519246e-03  2.70611402e-02  2.20353133e-03  3.78867891e-03\\n  1.29411439e-03  3.50290649e-02 -2.58361306e-02 -3.20287235e-02\\n  3.19325998e-02  5.17735779e-02  3.26043740e-02 -2.35525165e-02\\n -8.89074616e-03 -9.25663188e-02 -1.09475553e-02  6.18337803e-02\\n -9.19495523e-03 -1.52931651e-02 -2.32071225e-02  5.54587785e-03\\n  2.19747284e-03  4.17146422e-02  2.41861083e-02  5.54502122e-02\\n  1.29140131e-02  2.37725414e-02 -4.04298417e-02 -8.37784261e-03\\n  1.24115532e-03  3.72962430e-02 -2.39369404e-02  3.17287147e-02\\n -1.81639157e-02 -3.81305888e-02 -2.29516570e-02  1.86472125e-02\\n -2.51918212e-02 -1.11231739e-02  2.61019431e-02 -4.17887326e-03\\n  6.35215407e-03  3.05978372e-03 -2.26567276e-02  5.52211553e-02\\n  2.82943714e-02  5.34320474e-02  1.34907169e-02 -4.37364243e-02\\n  4.72911022e-04 -5.08616529e-02  2.24530231e-03  2.40628179e-02\\n -5.08143939e-02  3.10155284e-02  5.88588463e-03  6.03938997e-02\\n -6.95785042e-03  8.09021369e-02  1.35770431e-02  2.20059697e-02\\n  2.92207021e-03  3.32831107e-02 -2.36401837e-02  3.11954096e-02\\n -4.10429724e-02 -5.60786687e-02 -5.94614656e-04  2.23749820e-02\\n  2.54720799e-03 -1.74369328e-02  6.10900186e-02  5.14829457e-02\\n  2.62116641e-02  1.92390960e-02  7.50768988e-04  3.26913968e-02\\n  4.47159680e-03  4.62862989e-03 -2.28656530e-02 -1.22452769e-02\\n -1.07553555e-02  5.44747226e-02  3.25375088e-02 -8.92883632e-03\\n -2.65619420e-02  5.18130697e-02 -4.14081924e-02  1.72498990e-02\\n  2.43675858e-02 -6.26094043e-02 -1.49971270e-03  3.28977033e-02\\n -1.18482010e-02  1.05182640e-02 -1.13771232e-02  1.52540198e-02\\n -6.17332496e-02  5.94183467e-02 -1.74798481e-02  3.99899669e-02\\n -4.91301995e-03 -3.88532132e-02  4.38259244e-02  7.51819462e-04\\n -1.10452743e-02  3.96509208e-02 -5.72199374e-02  5.06886914e-02\\n -1.45355109e-02 -2.52195727e-02  4.76122275e-02 -5.30431755e-02\\n -4.65138350e-03 -1.32735260e-02  1.48337185e-02  1.04052663e-01\\n  3.78372557e-02 -1.66944489e-02  4.94301505e-02  2.64419690e-02\\n  3.87430713e-02 -2.47008689e-02  3.11886929e-02 -2.38935221e-02\\n -3.32879019e-03 -4.90695462e-02  3.12209334e-02  5.64975198e-03\\n -1.49829434e-02  4.12272103e-02 -1.28541626e-02 -4.14467268e-02\\n  2.29692366e-02  5.30691929e-02 -2.79648392e-03  9.76619944e-02\\n -3.33680725e-03 -3.79325971e-02  3.39732692e-02 -8.15867353e-03\\n  1.95176713e-02 -3.17266546e-02  2.76834685e-02  3.81082972e-03\\n  1.27146882e-03 -3.36397141e-02 -6.71808943e-02 -2.31624162e-03\\n  9.74480063e-03  7.14553520e-02  1.19599774e-02  7.30535947e-03\\n -2.38244166e-03 -4.34231386e-03  8.59754626e-03  7.95183238e-03\\n -6.88121393e-02  6.17698543e-02  2.05795504e-02  1.50338085e-02\\n -2.66217478e-02 -8.35669599e-03 -6.99697286e-02 -4.49123569e-02\\n -7.37115517e-02  3.19464058e-02 -3.07798814e-02 -7.66985044e-02\\n  1.89426709e-02  1.13014160e-02 -2.09322702e-02  4.37395684e-02\\n -3.37798335e-02 -3.42227519e-02 -1.77016798e-02 -2.69568264e-02\\n -3.63975391e-02  1.71616971e-02 -5.46425465e-04  6.61909534e-03\\n -5.62195061e-03  8.01925510e-02 -3.30427475e-02  5.89381903e-02\\n  1.32203279e-02  1.94528140e-02 -4.06433875e-03  6.62010442e-03\\n  2.55999155e-02 -2.27144100e-02  1.24016985e-01  1.62556879e-02\\n -1.74379665e-02 -1.78733207e-02  6.59452146e-03  1.21004969e-01\\n  4.74225320e-02 -9.45353508e-03  4.78293635e-02 -1.17366121e-03\\n -2.45884508e-02  4.98797453e-04 -2.94580702e-02  6.61749989e-02\\n  5.29057719e-03  4.71242182e-02 -5.08823025e-04 -4.14450187e-03\\n -3.93875204e-02 -2.48596538e-02 -1.20226685e-02  2.86598373e-02\\n  4.12970260e-02  4.82219532e-02  2.75197811e-02 -4.92078774e-02\\n -2.67136749e-02  9.12839454e-03 -5.45903407e-02 -7.02854292e-03\\n -1.63870286e-02  3.04527041e-02  2.88387425e-02  8.44170060e-03\\n  2.42968020e-03  1.03751123e-02 -4.05461993e-03  2.85729840e-02\\n -2.12687515e-02  8.71469732e-03  4.17382643e-02 -5.81028350e-02\\n -6.57838881e-02 -3.61579955e-02  8.71246029e-03  8.81481692e-02\\n  8.76042992e-03 -1.10896332e-02 -3.13027054e-02 -8.36808607e-03\\n  2.24093217e-02 -3.53497788e-02  4.64792177e-02  1.57019366e-02\\n -2.84721907e-02 -4.64542629e-03 -9.90463980e-03 -2.12735198e-02\\n  3.03749405e-02 -4.30260785e-02 -1.26876626e-02 -2.74391491e-02\\n  7.86173344e-02 -4.44015302e-02 -4.63078730e-02 -5.84436873e-33\\n  2.52901334e-02 -1.22533622e-03  2.77748099e-03 -1.93980560e-02\\n  6.36001350e-03  7.04465210e-02  5.31472862e-02  8.39251745e-03\\n -5.63005917e-02  2.05048844e-02  2.50587221e-02  1.99946109e-02\\n -9.76994867e-04  1.19377207e-02  3.38732712e-02  3.10741961e-02\\n -1.95467491e-02 -8.23809952e-02  3.13371085e-02 -6.07699994e-03\\n -4.58179228e-02 -2.34846137e-02  4.69333977e-02 -2.66870335e-02\\n  2.99764778e-02  4.57940623e-03  3.68848629e-02 -8.09239410e-03\\n -3.91403548e-02 -2.12850124e-02 -1.58816334e-02  5.18109091e-03\\n  4.49667830e-04  6.88075125e-02  1.90401971e-02  3.69306235e-03\\n -5.63388783e-03 -6.54179081e-02  2.01118906e-04 -4.79890518e-02\\n -8.07988346e-02  3.20624709e-02 -1.01095662e-02 -4.02874388e-02\\n  2.35207118e-02  2.24210862e-02 -5.00765536e-03  6.61188085e-03\\n  6.33936897e-02 -3.64622623e-02 -3.75311524e-02  6.13954151e-03\\n -1.46715362e-02 -1.08262962e-02 -3.08830980e-02  6.81238919e-02\\n -1.99181549e-02 -9.69508290e-03  2.02977508e-02  3.64061221e-02\\n  1.30685084e-02  4.41168854e-03 -9.40269157e-02  2.14329083e-02\\n -1.58167053e-02  2.95217764e-02  1.00607947e-02  8.51427391e-03\\n -5.09682670e-02 -5.59637323e-02 -5.46281338e-02  3.26886475e-02\\n -2.30248272e-03 -1.13981050e-02  4.67576943e-02 -2.70811394e-02\\n -5.65672144e-02 -7.20600132e-03  8.01948085e-02  3.82957384e-02\\n -5.66239432e-02  2.40409803e-02 -3.09104827e-04  6.30280981e-03\\n  2.36698873e-02 -1.47807449e-02  1.33694778e-03 -1.02467937e-02\\n -6.37239143e-02 -1.35340011e-02  7.17986654e-03  5.95078664e-03\\n  1.69184320e-02 -1.00788902e-02  3.31459269e-02 -1.39945187e-02\\n  2.43025329e-02 -1.34768318e-02  5.46660088e-02  3.35097574e-02\\n -4.18631593e-03  1.20559670e-02  9.96872690e-03 -8.02405830e-03\\n -2.07022242e-02 -1.35632381e-02  1.76398959e-02  4.72904788e-03\\n  2.84237582e-02  5.47406683e-03  5.50538115e-02 -3.87830101e-02\\n  2.52093542e-02 -2.19766260e-03 -1.40215801e-02 -2.50385376e-03\\n -8.00563022e-04 -4.19641584e-02 -1.19341845e-02  7.16386829e-03\\n -4.00717109e-02 -3.03082317e-02 -2.99562775e-02 -2.40452476e-02\\n  3.49001437e-02 -1.57152545e-02  1.98987331e-02  2.06106920e-02\\n -3.93997021e-02 -2.40752660e-02  9.95541923e-03  3.20616625e-02\\n  2.85939109e-07  1.87074784e-02 -1.40055991e-03 -3.99141610e-02\\n -5.14978766e-02  6.18599914e-02 -3.14667150e-02  1.45795029e-02\\n -9.48637165e-03 -4.80021276e-02 -1.23640768e-01 -7.49548431e-03\\n  4.88134511e-02  2.27913447e-02  4.93784482e-03 -2.99876612e-02\\n  3.39511037e-02 -1.36664929e-02 -5.97698502e-02  1.43400822e-02\\n  7.44066387e-03 -6.55861422e-02 -4.55697849e-02 -4.00813520e-02\\n -3.28516476e-02  4.69854334e-03  3.70964557e-02 -1.64579116e-02\\n  2.70081754e-03  3.53282616e-02 -3.39808613e-02  1.71928611e-02\\n -1.29510593e-02  3.41144986e-02  2.54638400e-02  9.78602213e-04\\n -5.00721298e-03 -6.41398877e-03 -1.76254585e-02 -2.70556696e-02\\n  5.34198470e-02 -2.46294774e-03 -5.98585606e-03  1.63509268e-02\\n -2.67965533e-02 -3.76960542e-03  7.36843678e-04  8.06827098e-03\\n -9.76282060e-02  7.08713159e-02  8.81878007e-03  3.66175249e-02\\n -5.96829504e-02 -4.19249712e-03 -4.77341749e-02  3.49609321e-03\\n -6.49406388e-02  1.75297447e-02  1.77128389e-02  3.39440480e-02\\n  9.04354751e-02 -1.21040661e-02 -1.56275537e-02 -1.67851686e-05\\n -4.02413867e-02  6.47976026e-02  4.05368023e-03  2.55564079e-02\\n  2.74016963e-34  3.57053764e-02 -1.24818478e-02 -8.30527209e-03\\n -3.16899531e-02 -1.62917208e-02 -8.44950415e-03  1.34990709e-02\\n -2.03760136e-02  9.23624169e-03  1.50190108e-03 -2.35626735e-02]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
        "\n",
        "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
        "\n",
        "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
        "\n",
        "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
        "embeddings.shape"
      ],
      "metadata": {
        "id": "FgPAmOiVndKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
        "                                      device=device)"
      ],
      "metadata": {
        "id": "INd7h6EloFzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_wrapped(text, wrap_length=80):\n",
        "    wrapped_text = textwrap.fill(text, wrap_length)\n",
        "    print(wrapped_text)"
      ],
      "metadata": {
        "id": "En9_vPl0rya0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
        "    print(f\"Score: {score:.4f}\")\n",
        "\n",
        "    print(\"Text:\")\n",
        "    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
        "    print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "EQ9H5UL0opnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_resources(query: str,\n",
        "                                embeddings: torch.tensor,\n",
        "                                model: SentenceTransformer=embedding_model,\n",
        "                                n_resources_to_return: int=5,\n",
        "                                print_time: bool=True):\n",
        "\n",
        "    query_embedding = model.encode(query,\n",
        "                                   convert_to_tensor=True)\n",
        "\n",
        "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
        "\n",
        "    scores, indices = torch.topk(input=dot_scores,\n",
        "                                 k=n_resources_to_return)\n",
        "\n",
        "    return scores, indices\n",
        "\n",
        "def print_top_results_and_scores(query: str,\n",
        "                                 embeddings: torch.tensor,\n",
        "                                 pages_and_chunks: list[dict]=pages_and_chunks,\n",
        "                                 n_resources_to_return: int=5):\n",
        "\n",
        "    scores, indices = retrieve_relevant_resources(query=query,\n",
        "                                                  embeddings=embeddings,\n",
        "                                                  n_resources_to_return=n_resources_to_return)\n",
        "\n",
        "    print(f\"Query: {query}\\n\")\n",
        "    print(\"Results:\")\n",
        "    for score, index in zip(scores, indices):\n",
        "        print(f\"Score: {score:.4f}\")\n",
        "        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n",
        "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "7SDDrGgGpONX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_quantization_config = True\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                         bnb_4bit_compute_dtype=torch.float16)\n",
        "\n",
        "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
        "  attn_implementation = \"flash_attention_2\"\n",
        "else:\n",
        "  attn_implementation = \"sdpa\"\n",
        "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
        "\n",
        "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
        "                                                 torch_dtype=torch.float16,\n",
        "                                                 quantization_config=quantization_config if use_quantization_config else None,\n",
        "                                                 low_cpu_mem_usage=False,\n",
        "                                                 attn_implementation=attn_implementation)\n",
        "\n",
        "if not use_quantization_config:\n",
        "    llm_model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "w75Wqf8EpzOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_formatter(query: str,\n",
        "                     context_items: list[dict]) -> str:\n",
        "\n",
        "    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
        "\n",
        "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
        "                      Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
        "                      Don't return the thinking, only return the answer.\n",
        "                      Make sure your answers are as explanatory as possible.\n",
        "                      Use the following examples as reference for the ideal answer style.\n",
        "                      \\nExample 1:\n",
        "                      Query: What are the fat-soluble vitamins?\n",
        "                      Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n",
        "                      \\nExample 2:\n",
        "                      Query: What are the causes of type 2 diabetes?\n",
        "                      Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n",
        "                      \\nExample 3:\n",
        "                      Query: What is the importance of hydration for physical performance?\n",
        "                      Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n",
        "                      \\nNow use the following context items to answer the user query:\n",
        "                      {context}\n",
        "                      \\nRelevant passages: <extract relevant passages from the context here>\n",
        "                      User query: {query}\n",
        "                      Answer:\"\"\"\n",
        "\n",
        "    base_prompt = base_prompt.format(context=context, query=query)\n",
        "\n",
        "    dialogue_template = [\n",
        "        {\"role\": \"user\",\n",
        "        \"content\": base_prompt}\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
        "                                          tokenize=False,\n",
        "                                          add_generation_prompt=True)\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "Iks4XQT9qUVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(query,\n",
        "        temperature=0.7,\n",
        "        max_new_tokens=512,\n",
        "        format_answer_text=True,\n",
        "        return_answer_only=True):\n",
        "\n",
        "    scores, indices = retrieve_relevant_resources(query=query,\n",
        "                                                  embeddings=embeddings)\n",
        "\n",
        "    context_items = [pages_and_chunks[i] for i in indices]\n",
        "\n",
        "    for i, item in enumerate(context_items):\n",
        "        item[\"score\"] = scores[i].cpu()\n",
        "\n",
        "    prompt = prompt_formatter(query=query,\n",
        "                              context_items=context_items)\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "\n",
        "    outputs = llm_model.generate(**input_ids,\n",
        "                                 temperature=temperature,\n",
        "                                 do_sample=True,\n",
        "                                 max_new_tokens=max_new_tokens)\n",
        "\n",
        "    output_text = tokenizer.decode(outputs[0])\n",
        "\n",
        "    if format_answer_text:\n",
        "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
        "\n",
        "    if return_answer_only:\n",
        "        return output_text\n",
        "\n",
        "    return output_text, context_items"
      ],
      "metadata": {
        "id": "FWXtI2kPqzQ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}